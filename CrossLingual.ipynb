{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSJOg0I5U1k-"
      },
      "source": [
        "# CA 4 - Part 2 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mPknbi_Bwag",
        "outputId": "0177fb31-74f6-4ff1-ceb9-0bb8e1872dcf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V2sjjKoFVdPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00b67df-a53c-4f54-dae1-89efeefe0bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 57.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_zg2S536VHtu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel, BertTokenizer\n",
        "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMgFaT_qUu6n"
      },
      "source": [
        "## Loading the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ReapDz90UJpR"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_excel('/content/drive/MyDrive/Sheyda/NLP_Data/train.xlsx')\n",
        "test_df = pd.read_excel('/content/drive/MyDrive/Sheyda/NLP_Data/test.xlsx')\n",
        "valid_df = pd.read_excel('/content/drive/MyDrive/Sheyda/NLP_Data/valid.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qufmMfdZUNZu",
        "outputId": "5cc29c10-8b81-4836-8301-58d7b04a01b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              source  \\\n",
              "0  When news is brought to one of them, of (the b...   \n",
              "1  After them repaired Zadok the son of Immer ove...   \n",
              "2  And establish regular prayers at the two ends ...   \n",
              "3  And it came to pass, that, when I was come aga...   \n",
              "4       Ah woe, that Day, to the Rejecters of Truth!   \n",
              "\n",
              "                                             targets category  \n",
              "0  و چون یکی از آنان را به [ولادت] دختر مژده دهند...    quran  \n",
              "1  و چون دشمنان ما شنیدند که ما آگاه شده‌ایم و خد...    bible  \n",
              "2  و نماز را در دو طرف روز و ساعات نخستین شب برپا...    quran  \n",
              "3  و فرمود تا مدعیانش نزد تو حاضر شوند؛ و از او ب...    bible  \n",
              "4                    وای در آن روز بر تکذیب کنندگان!    quran  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2aed798b-9ed7-42de-a81c-36eb7b515ab4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>targets</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When news is brought to one of them, of (the b...</td>\n",
              "      <td>و چون یکی از آنان را به [ولادت] دختر مژده دهند...</td>\n",
              "      <td>quran</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>After them repaired Zadok the son of Immer ove...</td>\n",
              "      <td>و چون دشمنان ما شنیدند که ما آگاه شده‌ایم و خد...</td>\n",
              "      <td>bible</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>And establish regular prayers at the two ends ...</td>\n",
              "      <td>و نماز را در دو طرف روز و ساعات نخستین شب برپا...</td>\n",
              "      <td>quran</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>And it came to pass, that, when I was come aga...</td>\n",
              "      <td>و فرمود تا مدعیانش نزد تو حاضر شوند؛ و از او ب...</td>\n",
              "      <td>bible</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ah woe, that Day, to the Rejecters of Truth!</td>\n",
              "      <td>وای در آن روز بر تکذیب کنندگان!</td>\n",
              "      <td>quran</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2aed798b-9ed7-42de-a81c-36eb7b515ab4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2aed798b-9ed7-42de-a81c-36eb7b515ab4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2aed798b-9ed7-42de-a81c-36eb7b515ab4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0PUsRVeUNXW",
        "outputId": "3ff32d8c-dff6-4fb3-eebf-b6e3d932df3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "source      0\n",
              "targets     0\n",
              "category    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#Missing values in training set\n",
        "train_df.isnull().sum()\n",
        "#Missing values in test set\n",
        "test_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQkemYatV4Mu",
        "outputId": "f7cce94c-7d0f-4a13-b25d-567dd92e43fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "quran    4200\n",
              "bible    4200\n",
              "mizan    4200\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_df.category.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxL-QeGnUNVH",
        "outputId": "0e7005b8-9433-4ef9-afb7-9aedf8010041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quran example : When news is brought to one of them, of (the birth of) a female (child), his face darkens, and he is filled with inward grief! و چون یکی از آنان را به [ولادت] دختر مژده دهند [از شدت خشم] چهره‌اش سیاه گردد، ودرونش از غصه واندوه لبریز و آکنده شود!!\n",
            "Bible example : After them repaired Zadok the son of Immer over against his house. After him repaired also Shemaiah the son of Shechaniah, the keeper of the east gate. و چون دشمنان ما شنیدند که ما آگاه شده‌ایم و خدا مشورت ایشان را باطل کرده است، آنگاه جمیع ما هر کس به‌کار خود به حصاربرگشتیم.\n",
            "Mizan example : This man had become a just man in the full force of the term. این مرد، بت مام معنی کلمه، یک مرد درستکار شده بود.\n"
          ]
        }
      ],
      "source": [
        "print(\"Quran example :\",train_df[train_df['category']=='quran']['source'].values[0], train_df[train_df['category']=='quran']['targets'].values[0])\n",
        "print(\"Bible example :\",train_df[train_df['category']=='bible']['source'].values[0], train_df[train_df['category']=='bible']['targets'].values[0])\n",
        "print(\"Mizan example :\",train_df[train_df['category']=='mizan']['source'].values[0], train_df[train_df['category']=='mizan']['targets'].values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DYn8DEMNUNS-"
      },
      "outputs": [],
      "source": [
        "train_df[\"category\"] = train_df[\"category\"].map({'mizan': 0, 'bible':1 , 'quran': 2})\n",
        "test_df[\"category\"] = test_df[\"category\"].map({'mizan': 0, 'bible':1 , 'quran': 2})\n",
        "valid_df[\"category\"] = valid_df[\"category\"].map({'mizan': 0, 'bible':1 , 'quran': 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFwl_s-nW2QP"
      },
      "source": [
        "## PART 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "znh-DswvWhcX"
      },
      "outputs": [],
      "source": [
        "# general config\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 32\n",
        "\n",
        "EPOCHS = 10\n",
        "EVERY_EPOCH = 500\n",
        "LEARNING_RATE = 3e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "67nuf2dqWhX_"
      },
      "outputs": [],
      "source": [
        "X_train_e = train_df['source']\n",
        "y_train_e = train_df['category']\n",
        "\n",
        "X_test_fa = test_df['targets']\n",
        "y_test_fa = test_df['category']\n",
        "\n",
        "X_eval_e = valid_df['source']\n",
        "y_eval_e = valid_df['category']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKMbTMJ5WhV2",
        "outputId": "d55f3cf5-e8c4-4c99-fa8e-db85cf5f98a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type xlm-roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing BertForSequenceClassification: ['roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'lm_head.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.key.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.weight', 'classifier.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.attention.self.key.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.value.weight', 'pooler.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'xlm-roberta-base', \n",
        "    num_labels = 3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "V6KFWgViWhT-"
      },
      "outputs": [],
      "source": [
        "X_train_e = X_train_e.values.tolist()\n",
        "X_test_fa = X_test_fa.values.tolist()\n",
        "\n",
        "y_train_e = y_train_e.values.tolist()\n",
        "y_test_fa = y_test_fa.values.tolist()\n",
        "\n",
        "X_eval_e = X_eval_e.values.tolist()\n",
        "y_eval_e = y_eval_e.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NAetyXu-WhRm"
      },
      "outputs": [],
      "source": [
        "train_encoding_e = tokenizer(X_train_e, padding=True, truncation=True, max_length=MAX_LEN, return_tensors = 'pt', return_attention_mask = True,)\n",
        "test_encoding_e = tokenizer(X_test_fa, padding=True, truncation=True, max_length=MAX_LEN, return_tensors = 'pt', return_attention_mask = True,)\n",
        "eval_encoding_e = tokenizer(X_eval_e, padding=True, truncation=True, max_length=MAX_LEN, return_tensors = 'pt', return_attention_mask = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cy81dsptZmNg"
      },
      "outputs": [],
      "source": [
        "class SourcedetectionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "## Test Dataset\n",
        "# class SourcedetectionTestDataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, encodings):\n",
        "#         self.encodings = encodings\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "#         return item\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaQG4tMTjSzg"
      },
      "source": [
        "## Generate Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2_L9tQfLZmLe"
      },
      "outputs": [],
      "source": [
        "train_dataset = SourcedetectionDataset(train_encoding_e, y_train_e)\n",
        "test_dataset = SourcedetectionDataset(test_encoding_e,y_test_fa)\n",
        "val_dataset = SourcedetectionDataset(eval_encoding_e, y_eval_e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset.encodings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CiVoMI3QyDJ",
        "outputId": "582695a5-761d-421d-857d-03d8c6cfc383"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[     0,    498,  10196,  ...,      1,      1,      1],\n",
              "        [     0,     65, 135218,  ...,      1,      1,      1],\n",
              "        [     0,   1765, 119957,  ...,      1,      1,      1],\n",
              "        ...,\n",
              "        [     0,     65,  15675,  ...,      1,      1,      1],\n",
              "        [     0,  22143,    870,  ...,      1,      1,      1],\n",
              "        [     0,     65,   1228,  ...,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyRTPfUQjvwA"
      },
      "source": [
        "## Define a Simple Metrics Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Hne3h2RFZmI-"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    #precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(labels, pred, average='weighted')\n",
        "\n",
        "    return {\"accuracy\": accuracy,\"f1_score\":f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FquK_TS2ZmDu"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='output',\n",
        "    save_strategy='epoch',\n",
        "    evaluation_strategy='epoch',\n",
        "    eval_steps=EVERY_EPOCH,\n",
        "    metric_for_best_model='accuracy',\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit = 1,\n",
        "    logging_steps = 20\n",
        "\n",
        ")\n",
        "training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QsKEvB3xjtNo"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    args=training_args, \n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qI6rBkLtjtLQ",
        "outputId": "08d5c953-1572-4f5b-f2b9-ebee4f9c222d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 12600\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3940\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3940' max='3940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3940/3940 30:23, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.233800</td>\n",
              "      <td>0.198822</td>\n",
              "      <td>0.928889</td>\n",
              "      <td>0.928456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.125900</td>\n",
              "      <td>0.145363</td>\n",
              "      <td>0.951852</td>\n",
              "      <td>0.951731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.083500</td>\n",
              "      <td>0.160744</td>\n",
              "      <td>0.956667</td>\n",
              "      <td>0.956672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.043600</td>\n",
              "      <td>0.182632</td>\n",
              "      <td>0.965185</td>\n",
              "      <td>0.965202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.036000</td>\n",
              "      <td>0.211184</td>\n",
              "      <td>0.962222</td>\n",
              "      <td>0.962258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.069900</td>\n",
              "      <td>0.195437</td>\n",
              "      <td>0.966296</td>\n",
              "      <td>0.966272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.030100</td>\n",
              "      <td>0.224663</td>\n",
              "      <td>0.964444</td>\n",
              "      <td>0.964479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>0.233516</td>\n",
              "      <td>0.964815</td>\n",
              "      <td>0.964903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.018500</td>\n",
              "      <td>0.236722</td>\n",
              "      <td>0.967037</td>\n",
              "      <td>0.967038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.241082</td>\n",
              "      <td>0.967037</td>\n",
              "      <td>0.967044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to output/checkpoint-394\n",
            "Configuration saved in output/checkpoint-394/config.json\n",
            "Model weights saved in output/checkpoint-394/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to output/checkpoint-788\n",
            "Configuration saved in output/checkpoint-788/config.json\n",
            "Model weights saved in output/checkpoint-788/pytorch_model.bin\n",
            "Deleting older checkpoint [output/checkpoint-394] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to output/checkpoint-1182\n",
            "Configuration saved in output/checkpoint-1182/config.json\n",
            "Model weights saved in output/checkpoint-1182/pytorch_model.bin\n",
            "Deleting older checkpoint [output/checkpoint-788] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to output/checkpoint-1576\n",
            "Configuration saved in output/checkpoint-1576/config.json\n",
            "Model weights saved in output/checkpoint-1576/pytorch_model.bin\n",
            "Deleting older checkpoint [output/checkpoint-1182] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to output/checkpoint-1970\n",
            "Configuration saved in output/checkpoint-1970/config.json\n",
            "Model weights saved in output/checkpoint-1970/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to output/checkpoint-2364\n",
            "Configuration saved in output/checkpoint-2364/config.json\n",
            "Model weights saved in output/checkpoint-2364/pytorch_model.bin\n",
            "Deleting older checkpoint [output/checkpoint-1576] due to args.save_total_limit\n",
            "Deleting older checkpoint [output/checkpoint-1970] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to output/checkpoint-2758\n",
            "Configuration saved in output/checkpoint-2758/config.json\n",
            "Model weights saved in output/checkpoint-2758/pytorch_model.bin\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to output/checkpoint-3152\n",
            "Configuration saved in output/checkpoint-3152/config.json\n",
            "Model weights saved in output/checkpoint-3152/pytorch_model.bin\n",
            "Deleting older checkpoint [output/checkpoint-2758] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to output/checkpoint-3546\n",
            "Configuration saved in output/checkpoint-3546/config.json\n",
            "Model weights saved in output/checkpoint-3546/pytorch_model.bin\n",
            "Deleting older checkpoint [output/checkpoint-2364] due to args.save_total_limit\n",
            "Deleting older checkpoint [output/checkpoint-3152] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to output/checkpoint-3940\n",
            "Configuration saved in output/checkpoint-3940/config.json\n",
            "Model weights saved in output/checkpoint-3940/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from output/checkpoint-3546 (score: 0.967037037037037).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3940, training_loss=0.10013755348060152, metrics={'train_runtime': 1824.4286, 'train_samples_per_second': 69.063, 'train_steps_per_second': 2.16, 'total_flos': 8288072658432000.0, 'train_loss': 0.10013755348060152, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "8STyKGYbmrqB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5266a562-7d2d-4814-c602-84932ebdb6f3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 00:09]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 10.0,\n",
              " 'eval_accuracy': 0.967037037037037,\n",
              " 'eval_f1_score': 0.9670384888422484,\n",
              " 'eval_loss': 0.23672173917293549,\n",
              " 'eval_runtime': 10.1187,\n",
              " 'eval_samples_per_second': 266.833,\n",
              " 'eval_steps_per_second': 8.4}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.predict(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ismYbmY1WuhC",
        "outputId": "fec869e7-594c-4dc6-ff21-966fd59ff4be"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='170' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 00:20]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[ 6.294245 , -4.042062 , -3.2287474],\n",
              "       [ 5.922205 , -4.5160885, -2.3854113],\n",
              "       [ 5.90173  , -2.3339765, -4.5932097],\n",
              "       ...,\n",
              "       [ 6.316438 , -4.2778273, -3.0126653],\n",
              "       [ 6.454144 , -3.911014 , -3.5302792],\n",
              "       [ 6.1530776, -4.345872 , -2.770504 ]], dtype=float32), label_ids=array([0, 1, 1, ..., 2, 0, 1]), metrics={'test_loss': 6.341509819030762, 'test_accuracy': 0.33185185185185184, 'test_f1_score': 0.17940285904432687, 'test_runtime': 10.0753, 'test_samples_per_second': 267.983, 'test_steps_per_second': 8.436})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pin_memory=False\n",
        "preds = trainer.predict(test_dataset=test_dataset)\n",
        "preds"
      ],
      "metadata": {
        "id": "U21G7FNBmrkC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "61f62d24-227f-4f25-ea3a-0432a1de6280"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 2700\n",
            "  Batch size = 32\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='255' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 00:30]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[ 6.294245 , -4.042062 , -3.2287474],\n",
              "       [ 5.922205 , -4.5160885, -2.3854113],\n",
              "       [ 5.90173  , -2.3339765, -4.5932097],\n",
              "       ...,\n",
              "       [ 6.316438 , -4.2778273, -3.0126653],\n",
              "       [ 6.454144 , -3.911014 , -3.5302792],\n",
              "       [ 6.1530776, -4.345872 , -2.770504 ]], dtype=float32), label_ids=array([0, 1, 1, ..., 2, 0, 1]), metrics={'test_loss': 6.341509819030762, 'test_accuracy': 0.33185185185185184, 'test_f1_score': 0.17940285904432687, 'test_runtime': 10.0742, 'test_samples_per_second': 268.011, 'test_steps_per_second': 8.437})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds.metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idzYbtwvX_xb",
        "outputId": "afa0a700-771f-458f-dfa1-fae20743f2ac"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_accuracy': 0.33185185185185184,\n",
              " 'test_f1_score': 0.17940285904432687,\n",
              " 'test_loss': 6.341509819030762,\n",
              " 'test_runtime': 10.0742,\n",
              " 'test_samples_per_second': 268.011,\n",
              " 'test_steps_per_second': 8.437}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = torch.from_numpy(preds[0]).softmax(1)\n",
        "\n",
        "# convert tensors to numpy array\n",
        "predictions = probs.numpy()"
      ],
      "metadata": {
        "id": "uKCKxnkIOuvR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0][0]/(predictions[0][0]+(0.5*(predictions[0][1] + predictions[1][0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YVVmwsITeuC",
        "outputId": "895474aa-5c26-406a-d5d9-f2297a01c74b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666973080985443"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_hkWpC3WECB",
        "outputId": "73de03c7-dc3d-413f-80a8-363ac34f9f3b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[ 6.294245 , -4.042062 , -3.2287474],\n",
              "       [ 5.922205 , -4.5160885, -2.3854113],\n",
              "       [ 5.90173  , -2.3339765, -4.5932097],\n",
              "       ...,\n",
              "       [ 6.316438 , -4.2778273, -3.0126653],\n",
              "       [ 6.454144 , -3.911014 , -3.5302792],\n",
              "       [ 6.1530776, -4.345872 , -2.770504 ]], dtype=float32), label_ids=array([0, 1, 1, ..., 2, 0, 1]), metrics={'test_loss': 6.341509819030762, 'test_accuracy': 0.33185185185185184, 'test_f1_score': 0.17940285904432687, 'test_runtime': 10.0742, 'test_samples_per_second': 268.011, 'test_steps_per_second': 8.437})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newdf = pd.DataFrame(predictions,columns=['0','1','2'])"
      ],
      "metadata": {
        "id": "BjAYTWZdZgxk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = np.argmax(predictions,axis=1)\n",
        "newdf['labels'] = results\n",
        "newdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BRTtfZ4iZl36",
        "outputId": "7740f32b-95e7-4c09-c8f3-a4c829b253bd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2  labels\n",
              "0     0.999894  0.000032  0.000073       0\n",
              "1     0.999724  0.000029  0.000247       0\n",
              "2     0.999707  0.000265  0.000028       0\n",
              "3     0.999882  0.000030  0.000088       0\n",
              "4     0.999923  0.000030  0.000048       0\n",
              "...        ...       ...       ...     ...\n",
              "2695  0.999896  0.000063  0.000041       0\n",
              "2696  0.999880  0.000024  0.000095       0\n",
              "2697  0.999886  0.000025  0.000089       0\n",
              "2698  0.999922  0.000032  0.000046       0\n",
              "2699  0.999839  0.000028  0.000133       0\n",
              "\n",
              "[2700 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c4f0dd6-97ab-476c-be06-dc02d1e71ad9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.999894</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.999724</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000247</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.999707</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.999882</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.999923</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2695</th>\n",
              "      <td>0.999896</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2696</th>\n",
              "      <td>0.999880</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2697</th>\n",
              "      <td>0.999886</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2698</th>\n",
              "      <td>0.999922</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2699</th>\n",
              "      <td>0.999839</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2700 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c4f0dd6-97ab-476c-be06-dc02d1e71ad9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c4f0dd6-97ab-476c-be06-dc02d1e71ad9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c4f0dd6-97ab-476c-be06-dc02d1e71ad9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "report = classification_report(y_test_fa, newdf['labels'], output_dict=True, zero_division=0)\n",
        "report['auc_score'] = roc_auc_score(y_test_fa, probs, multi_class='ovr')"
      ],
      "metadata": {
        "id": "VOSk1Cw2TemS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3kAlT-MaJgy",
        "outputId": "34337fd4-5403-494a-8f15-293556008b19"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': {'f1-score': 0.4978711325574794,\n",
              "  'precision': 0.3343499809378574,\n",
              "  'recall': 0.9744444444444444,\n",
              "  'support': 900},\n",
              " '1': {'f1-score': 0.01756311745334797,\n",
              "  'precision': 0.7272727272727273,\n",
              "  'recall': 0.008888888888888889,\n",
              "  'support': 900},\n",
              " '2': {'f1-score': 0.022774327122153208,\n",
              "  'precision': 0.16666666666666666,\n",
              "  'recall': 0.012222222222222223,\n",
              "  'support': 900},\n",
              " 'accuracy': 0.33185185185185184,\n",
              " 'auc_score': 0.5771768518518519,\n",
              " 'macro avg': {'f1-score': 0.17940285904432685,\n",
              "  'precision': 0.4094297916257505,\n",
              "  'recall': 0.33185185185185184,\n",
              "  'support': 2700},\n",
              " 'weighted avg': {'f1-score': 0.17940285904432687,\n",
              "  'precision': 0.4094297916257505,\n",
              "  'recall': 0.33185185185185184,\n",
              "  'support': 2700}}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kdPSbEi5FOp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jmbwz2Q3FOnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H19oAMnzEhcX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "CA4_Part3-.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}